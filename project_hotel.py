# -*- coding: utf-8 -*-
"""Project_hotel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w5Dd-S4KA9k4SeTPbWRXhXOvDPW7Tlu4

# OPIM 5512-  Data Science using Python, Spring 2024

## Hotel Booking Cancellation Analysis

```
Madhavi Muppani
Sai Teja Meka
Khyathi Manne
Caitlyn Kane
Pranathi Karne
Steeven Raj Burugula
```
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn.metrics import r2_score

from sklearn.metrics import roc_curve, auc
import matplotlib.pyplot as plt

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv("/content/drive/MyDrive/hotel_bookings.csv")

data.head()

data.tail()

data.shape

# Create a list of categorical columns. These are the columns in the dataframe 'data'
cat_col = [col for col in data.columns if data[col].dtype == 'object']
print('Categorical columns :', cat_col)

# Create a list of numerical columns. These are the columns in the dataframe 'data'
num_col = [col for col in data.columns if data[col].dtype != 'object']
print('Numerical columns :', num_col)

data.info()

data.columns

# Generating descriptive statistics for all columns

data.describe(include='all').T

# Count the occurrences of each unique value in the 'hotel' column of the DataFrame 'data'.

data.hotel.value_counts()

# Check for missing values in each column of the DataFrame 'data'.
data.isnull().sum()

# Calculate the percentage of missing values in each column of the DataFrame 'data'.
(data.isnull().sum() / len(data)) * 100

"""# Preprocessing of data"""

# dropping columns
data.drop(['company', 'country', 'reservation_status', 'assigned_room_type'], inplace=True, axis=1)

# Dropping the 'reservation_status_date' column from the 'data' DataFrame.
data.drop(['reservation_status_date'], inplace=True, axis=1)

# Fill missing values in the 'children' column with 0
data['children'].fillna(0, inplace=True)

# converting categorical agent variable to binary variable.
data['agent'] = data['agent'].notnull().astype(int)
data.head()

# combing children and babies to new column into binary variables and dropping indiviudals
data['kids'] = ((data['children'] > 0) | (data['babies'] > 0)).astype(int)
data.drop(['children', 'babies'], inplace=True, axis=1)

# data verification
# check rows with adults and kids are 0
data.loc[(data.adults == 0) & (data.kids == 0)]

# dropping out the above conditional rows
data = data.drop(data[(data['adults'] == 0) & (data['kids'] == 0)].index)

data

# Calculate the low and high quantiles (0.1% and 99.9%) of the 'lead_time' column to identify outliers.
q_low = data["lead_time"].quantile(0.001)
q_hi = data["lead_time"].quantile(0.999)

# Filter the data to exclude outliers beyond the defined quantiles.
data = data[(data["lead_time"] < q_hi) & (data["lead_time"] > q_low)]

# Calculate the low and high quantiles (0.1% and 99.9%) of the 'adr' (average daily rate) column to identify outliers.
q_low = data["adr"].quantile(0.001)
q_hi = data["adr"].quantile(0.999)

# Filter the data to exclude outliers beyond the defined quantiles in the 'adr' column.
data = data[(data["adr"] < q_hi) & (data["adr"] > q_low)]

"""# Visualizations"""

#heatmap
plt.figure(figsize = (15,15))
sns.heatmap(data.select_dtypes(include = 'number').corr().round(3),annot= True)

# Plotting a pie chart to visualize the distribution of 'is_canceled' values.

data['is_canceled'].value_counts().plot(kind='pie', autopct='%0.1f%%')

# Plotting a pie chart to visualize the distribution of hotel types.

data['hotel'].value_counts().plot(kind='pie', autopct='%0.1f%%')

# Plotting a histogram to visualize the distribution of lead times.

data["lead_time"].hist(figsize=(8, 5))

# Plotting a histogram to visualize the distribution of the Average Daily Rate (ADR).

data["adr"].hist(figsize=(8, 5))

# Plotting a histogram to visualize the distribution of the number of days in the waiting list.

data["days_in_waiting_list"].hist(figsize=(8, 5))

# Plotting a histogram to visualize the distribution of the total number of special requests made by guests.

data["total_of_special_requests"].hist(figsize=(8, 5))

# Plotting a histogram to visualize the distribution of the number of weeknights stayed.

data["stays_in_week_nights"].hist(figsize=(8, 5))

# Plotting a histogram to visualize the distribution of the number of weekend nights stayed.

data["stays_in_weekend_nights"].hist(figsize=(8, 5))

plt.figure(figsize=(8, 6))

# Generating a box plot to visualize the distribution of lead time based on cancellation status.

sns.boxplot(x='is_canceled', y='lead_time', data=data)
plt.xlabel('is_canceled')
plt.ylabel('lead_time')
plt.title('Box Plot of lead_time by is_canceled')

plt.figure(figsize=(8, 6))

# Generating a box plot to visualize the distribution of stays in weekend nights based on cancellation status.

sns.boxplot(x='is_canceled', y='stays_in_weekend_nights', data=data)
plt.xlabel('is_canceled')
plt.ylabel('stays_in_weekend_nights')
plt.title('Box Plot of stays_in_weekend_nights by is_canceled')

plt.figure(figsize=(8, 6))

# Generating a box plot to visualize the distribution of stays in weeknights based on cancellation status.

sns.boxplot(x='is_canceled', y='stays_in_week_nights', data=data)
plt.xlabel('is_canceled')
plt.ylabel('stays_in_week_nights')
plt.title('Box Plot of stays_in_week_nights by is_canceled')

plt.figure(figsize=(8, 6))

# Generating a box plot to visualize the distribution of Average Daily Rate (ADR) based on cancellation status.

sns.boxplot(x='is_canceled', y='adr', data=data)
plt.xlabel('is_canceled')
plt.ylabel('adr')
plt.title('Box Plot of adr by is_canceled')

plt.figure(figsize=(8, 6))

# Generating a box plot to visualize the distribution of days spent in the waiting list based on cancellation status.
sns.boxplot(x='is_canceled', y='days_in_waiting_list', data=data)
plt.xlabel('is_canceled')
plt.ylabel('days_in_waiting_list')
plt.title('Box Plot of days_in_waiting_list by is_canceled')

# Plotting a bar chart to visualize the count of canceled and non-canceled bookings.

data['is_canceled'].value_counts().plot(kind='bar')

# Plotting a pie chart to visualize the proportion of canceled and non-canceled bookings.

data['is_canceled'].value_counts().plot(kind='pie', autopct='%0.1f%%')

# Plotting a kernel density estimate (KDE) plot to visualize the distribution of lead times.

data['lead_time'].plot(kind='kde')

# Plotting a kernel density estimate (KDE) plot to visualize the distribution of Average Daily Rate (ADR).

data['adr'].plot(kind='kde')

# Plotting a bar chart to visualize the count of each market segment.

data['market_segment'].value_counts().plot(kind='bar')

summary_table = pd.crosstab(data['hotel'], data['is_canceled'])

# Plotting a grouped bar chart to visualize the cancellation status by hotel type.
summary_table.plot(kind='bar', stacked=False)
plt.title('Cancellation Status by Hotel Type')
plt.xlabel('Hotel Type')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Cancellation Status')
plt.show()

summary_table1 = pd.crosstab(data['hotel'], data['is_canceled'], normalize='index')
summary_table1 *= 100

# Plotting a grouped bar chart to visualize the cancellation percentage by hotel type.
summary_table1.plot(kind='bar', stacked=False)
plt.title('Cancellation Percentage by Hotel Type')
plt.xlabel('Hotel Type')
plt.ylabel('Percentage (%)')
plt.xticks(rotation=0)
plt.legend(title='Cancellation Status', labels=['Not Canceled', 'Canceled'])
plt.show()

plt.figure(figsize=(15, 20))
summary_table = pd.crosstab(data['market_segment'], data['is_canceled'])

# Plotting a grouped bar chart to visualize the cancellation status by market segment
summary_table.plot(kind='bar', stacked=False)
plt.title('Cancellation Status by Market Segment')
plt.xlabel('Market Segment')
plt.ylabel('Count')
plt.legend(title='Cancellation Status')
plt.tight_layout()
plt.show()

plt.figure(figsize=(20, 20))
summary_table2 = pd.crosstab(data['market_segment'], data['is_canceled'], normalize="index")
summary_table2 *= 100

# Plotting a grouped bar chart to visualize the cancellation percentage by market segment.
summary_table2.plot(kind='bar', stacked=False)
plt.title('Cancellation Percentage by Market Segment')
plt.xlabel('Market Segment')
plt.ylabel('Percentage (%)')
plt.legend(title='Cancellation Status', labels=['Not Canceled', 'Canceled'])
plt.show()

summary_table = pd.crosstab(data['deposit_type'], data['is_canceled'])
summary_table.plot(kind='bar', stacked=False)
plt.title('Cancellation Status by Deposit Type')
plt.xlabel('Deposit Type')
plt.ylabel('Count')
plt.xticks(rotation=0)
plt.legend(title='Cancellation Status')
plt.show()

plt.figure(figsize=(100, 100))
summary_table = pd.crosstab(data['arrival_date_month'], data['is_canceled'])
summary_table.plot(kind='bar', stacked=False)
plt.title('Cancellation Status by Arrival Date Month')
plt.xlabel('Arrival Date Month')
plt.ylabel('Count')
plt.legend(title='Cancellation Status', labels=['Not Canceled', 'Canceled'])
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 20))
summary_table = pd.crosstab(data['customer_type'], data['is_canceled'])

# Plotting a grouped bar chart to visualize the cancellation status by customer type.
summary_table.plot(kind='bar', stacked=False)
plt.title('Cancellation Status by Customer Type')
plt.xlabel('Customer Type')
plt.ylabel('Count')
plt.legend(title='Cancellation Status', labels=['Not Canceled', 'Canceled'])
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 20))
summary_table5 = pd.crosstab(data['customer_type'], data['is_repeated_guest'], normalize="index")
summary_table5 *= 100

# Plotting a grouped bar chart to visualize the percentage of repeated guests by customer type.
summary_table.plot(kind='bar', stacked=False)
plt.xlabel('Customer Type')
plt.ylabel('%')
plt.legend(title='is_repeated_guest')
plt.tight_layout()
plt.show()

plt.figure(figsize=(15, 20))
summary_table4 = pd.crosstab(data['is_repeated_guest'], data['is_canceled'], normalize="index")
summary_table4 *= 100

# Plotting a grouped bar chart to visualize the percentage of canceled and non-canceled bookings by is_repeated_guest.
summary_table4.plot(kind='bar', stacked=False)
plt.title('Cancellation Status by is_repeated_guest')
plt.xlabel('is_repeated_guest')
plt.ylabel('%')
plt.xticks(rotation=0)
plt.legend(title='Cancellation Status', labels=['Not Canceled', 'Canceled'])
plt.tight_layout()
plt.show()

# box plot of price based on reserved room_type
df = data[data['is_canceled'] == 0]

px.box(data_frame = df, x = 'reserved_room_type', y = 'adr', color = 'hotel', template = 'plotly_dark')

# filtering out resort and city hotel types
data_resort = data[(data['hotel'] == 'Resort Hotel') & (df['is_canceled'] == 0)]
data_city = data[(data['hotel'] == 'City Hotel') & (df['is_canceled'] == 0)]

# price of resort hotel type per month
resort_hotel = data_resort.groupby(['arrival_date_month'])['adr'].mean().reset_index()
# price of city hotel type per month
city_hotel=data_city.groupby(['arrival_date_month'])['adr'].mean().reset_index()
months_order = ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']

# Convert the arrival_date_month to a Categorical data type with the order defined
resort_hotel['arrival_date_month'] = pd.Categorical(resort_hotel['arrival_date_month'], categories=months_order, ordered=True)
city_hotel['arrival_date_month'] = pd.Categorical(city_hotel['arrival_date_month'], categories=months_order, ordered=True)
# Sort the DataFrame based on the ordered categorical
resort_hotel = resort_hotel.sort_values('arrival_date_month')
city_hotel = city_hotel.sort_values('arrival_date_month')

plt.figure(figsize=(8, 6))

# Plotting a line chart to visualize the monthly average daily rate (ADR) for the resort hotel.

plt.plot(resort_hotel['arrival_date_month'], resort_hotel['adr'], marker='o')
plt.title('Monthly Average Daily Rate (ADR) for Resort Hotel')
plt.xlabel('Month')
plt.ylabel('Average Daily Rate (ADR)')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))

# Plotting a line chart to visualize the monthly average daily rate (ADR) for the city hotel.

plt.plot(city_hotel['arrival_date_month'], city_hotel['adr'], marker='o')
plt.title('Monthly Average Daily Rate (ADR) for City Hotel')
plt.xlabel('Month')
plt.ylabel('Average Daily Rate (ADR)')
plt.grid(True)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""# Encoding the categorical varibles

"""

# mapping month names to their corresponding numerical values.
month_mapping = {
    'January': 1,
    'February': 2,
    'March': 3,
    'April': 4,
    'May': 5,
    'June': 6,
    'July': 7,
    'August': 8,
    'September': 9,
    'October': 10,
    'November': 11,
    'December': 12
}

data['arrival_date_month'] = data['arrival_date_month'].map(month_mapping)

categorical_columns = data.select_dtypes(include=['object'])

# Create a new DataFrame 'categorical_df' containing only the selected categorical columns.
categorical_df = pd.DataFrame(categorical_columns)

# Retrieve the names of the columns in the DataFrame 'categorical_df', which contains only categorical columns.
categorical_names = categorical_df.columns

categorical_names

# Convert categorical variables into dummy/indicator variables using one-hot encoding.
data_filt = pd.get_dummies(data, columns=categorical_names, drop_first=True)

"""# Splitting data
# Building Models
"""

data_filt.shape

X = data_filt.drop(['is_canceled'], axis=1)
y = data_filt['is_canceled']

# Split the data into training and test sets, with 20% of the data reserved for testing and stratifying by 'y'
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, stratify=y)

X_train

print("NaNs in X_train:", np.isnan(X_train).any())
print("NaNs in X_test:", np.isnan(X_test).any())

# Replace any NaN values in X_train with the column-wise median of the training data
X_train = np.where(np.isnan(X_train), np.nanmedian(X_train, axis=0), X_train)

# Replace any NaN values in X_test with the column-wise median of the test data
X_test = np.where(np.isnan(X_test), np.nanmedian(X_test, axis=0), X_test)

y_train

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)

# Initialize the StandardScaler to standardize features by removing the mean and scaling to unit variance
sc = StandardScaler()

# Fit the scaler to the training data to compute the mean and standard deviation for later scaling
sc.fit(X_train)

# Transform the training data using the computed mean and standard deviation to standardize it
X_train_std = sc.transform(X_train)

# Apply the same transformation to the test data to ensure consistency in feature scaling
X_test_std = sc.transform(X_test)

from sklearn.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
dt.fit(X_train_std, y_train)

# Predict the class labels for the test set
y_pred = dt.predict(X_test_std)

# Import accuracy_score to compute the subset accuracy
from sklearn.metrics import accuracy_score
print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))

from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Retrieve and store the feature importances from the fitted  decision tree model
importances = dt.feature_importances_

# Sort the indices of the features based on their importance in descending order
indices = np.argsort(importances)[::-1]
feat_labels = data_filt.columns[1:]

# Create a list of feature labels sorted according to their importance, determined previously
sorted_feat_labels = [feat_labels[i] for i in indices]

# Define the number of top features to select
top_n = 5

# Slice the sorted feature labels and their corresponding importances to get the top 5
sorted_feat_labels = sorted_feat_labels[:top_n]
importances = importances[indices[:top_n]]

plt.figure(figsize=(8, 6))
plt.title('Top 5 Feature Importances by DecisionTreeClassifier')
plt.bar(range(top_n), importances, align='center')
plt.xticks(range(top_n), sorted_feat_labels, rotation=90)
plt.xlim([-1, top_n])
plt.tight_layout()
plt.show()

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(C=100000.0, random_state=0, max_iter=10000)

# Fit the Logistic Regression model on the standardized training data
lr.fit(X_train_std, y_train)

# Predict the class labels for the standardized test data
y_pred = lr.predict(X_test_std)

# Calculate the accuracy score to assess the overall correctness of the model
from sklearn.metrics import accuracy_score
print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))

# Generate the confusion matrix and classification report to evaluate the performance of the classification
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

"""# Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

# Initialize the Gradient Boosting classifier
gb = GradientBoostingClassifier()

# Fit the model to the training data
gb.fit(X_train_std, y_train)  # X_train: features for training, y_train: labels for training

# Predict the labels for the test dataset
y_pred_gb = gb.predict(X_test_std)  # X_test: features for testing

# Calculate the accuracy of the model on the test data
acc_gb = accuracy_score(y_test, y_pred_gb)  # y_test: true labels for testing

# Generate the confusion matrix for the test data predictions
conf = confusion_matrix(y_test, y_pred_gb)  # Confusion matrix shows true vs predicted labels


clf_report = classification_report(y_test, y_pred_gb)  # Includes precision, recall, f1-score, etc.
print(f"Accuracy Score of Gradient Boosting Classifier is : {acc_gb}")
print(f"Confusion Matrix : \n{conf}")
print(f"Classification Report : \n{clf_report}")

from sklearn.linear_model import Perceptron
# Initialize the Perceptron with a specific number of iterations, a learning rate, and a random state for reproducibility
ppn = Perceptron(max_iter=100, eta0=0.1, random_state=0)

# Fit the Perceptron model on the standardized training data
ppn.fit(X_train_std, y_train)

# Use the trained Perceptron model to make predictions on the standardized test data
y_pred = ppn.predict(X_test_std)


from sklearn.metrics import accuracy_score
print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
# Define feature labels, excluding the first column which typically might be an index or non-feature column
feat_labels = data_filt.columns[1:]

# Initialize the Random Forest classifier with 100 trees, enabling parallel computation and a fixed random state for reproducibility
forest = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)

# Fit the Random Forest model to the training data
forest.fit(X_train, y_train)
y_pred = forest.predict(X_test_std)
from sklearn.metrics import accuracy_score
print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))
from sklearn.metrics import classification_report, confusion_matrix
print(confusion_matrix(y_test, y_pred))
print(classification_report(y_test, y_pred))

# Drop the 'is_canceled' column from the data_filt DataFrame and store the result in X.
X = data_filt.drop(['is_canceled'], axis=1)

# Extract the 'is_canceled' column from the data_filt DataFrame and store it in y.
y = data_filt['is_canceled']

# Convert the DataFrame X to a NumPy array and store it in K_X.
K_X = X.values
# Convert the Series y to a NumPy array and store it in K_y.
K_y = y.values

from sklearn.model_selection import train_test_split

# X_train and y_train are the features and target for the training set.
# X_test and y_test are the features and target for the test set.
X_train, X_test, y_train, y_test = \
train_test_split(K_X, K_y, test_size=0.20, random_state=1)

# K-Fold
# Stratified K-fold cross-validation ("stratified": % of each label same in each fold)
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.pipeline import Pipeline
from sklearn.model_selection import StratifiedKFold
skf = StratifiedKFold(n_splits=10)

# Compute the number of splits for the StratifiedKFold object using the training data
skf.get_n_splits(X_train, y_train)

"""# Pipeline"""

from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
pipe_lr = Pipeline([
    ('scl', StandardScaler()),
    ('clf', LogisticRegression(random_state=1))
])

scores = []
k = 0

# Execute a loop over the folds, provided by the StratifiedKFold object
for train, test in skf.split(X_train, y_train):
    k += 1  # Increment the fold number


    pipe_lr.fit(X_train[train], y_train[train])

    score = pipe_lr.score(X_train[test], y_train[test])

    scores.append(score)


    print('Fold: %s, Class dist.: %s, Acc: %.3f' % (k, np.bincount(y_train[train]), score))

from sklearn.model_selection import cross_val_score

# Perform k-fold cross-validation
scores = cross_val_score(estimator=pipe_lr,  # estimator: the model to evaluate, here 'pipe_lr' which seems to be a pipeline
                         X=X_train,          # X_train: the training data features
                         y=y_train,          # y_train: the training data labels
                         cv=10,              # cv: number of folds in k-fold cross-validation
                         n_jobs=1)           # n_jobs: number of CPU cores to use (1 means only one core)

print('CV accuracy scores: %s' % scores)
print('CV accuracy: %.3f +/- %.3f' % (np.mean(scores), np.std(scores)))